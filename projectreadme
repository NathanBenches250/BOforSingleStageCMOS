# OpAmp Circuit Parameter Optimization using Bayesian Optimization with Surrogate Models

Paper Reference: https://arxiv.org/html/2406.18770v1

## Introduction
This project addresses the challenge of optimizing operational amplifier (OpAmp) circuit parameters using Bayesian Optimization (BO) techniques enhanced with surrogate models. Circuit design optimization traditionally relies on time-consuming LTSpice simulations that can take seconds to minutes per evaluation, making the optimization process computationally expensive when hundreds or thousands of evaluations are needed.

The significance of this work lies in automating the circuit design process while reducing the computational cost through intelligent sampling strategies. By implementing surrogate models that can predict circuit performance metrics without running a full simulation, we can significantly accelerate the optimization process. This project is directly related to the course content on Bayesian optimization, acquisition functions, and surrogate modeling.

## Formal Problem Statement
We aim to optimize the parameters of a single-stage CMOS operational amplifier to maximize gain while minimizing power consumption. 

### Components of the Bayesian Optimization Setup:
- **Search Space**: Continuous parameter space consisting of 9 parameters:
  - Transistor widths (W_M1 through W_M6): 1.0-5.0 μm for M1-M4; 2.0-10.0 μm for M5-M6
  - Bias current (I1): 10-100 μA
  - Capacitances (C1, C2): 1-10 pF

- **Objective Function**: Score = -Gain + α·Power, where α=0.2 is a weighting factor
  - The negative sign for gain indicates we want to maximize gain
  - The positive coefficient for power indicates we want to minimize power
  - Lower scores represent better designs

- **Evaluation Cost**: Each LTSpice simulation takes approximately 5-10 seconds to run
  
- **Constraints**: Valid circuit configurations must have positive gain

- **Differences from Class Setup**: 
  1. We use two different surrogate models rather than a single model:
     - Gaussian Process for gain prediction
     - Random Forest for power prediction (due to its non-Gaussian distribution)
  2. We implement a custom Expected Improvement (EI) acquisition function that combines predictions from both models
  3. We use a two-stage approach where we pretrain surrogate models on random samples before starting the optimization process

## Approach
Our approach combines traditional Bayesian Optimization with specialized surrogate models to address the circuit optimization problem:

1. **Data Collection and Preprocessing**:
   - Generated initial random samples of circuit parameters
   - Ran LTSpice simulations to measure gain and power consumption
   - Preprocessed data, including log-transforming power values due to their wide range

2. **Surrogate Model Development**:
   - Implemented a Gaussian Process Regression (GPR) model with a customized kernel for gain prediction
   - Used a Random Forest Regressor for power prediction due to its non-Gaussian distribution
   - Trained both models on the initial dataset
   
   **Detailed Surrogate Model Analysis**:
   
   *Gaussian Process for Gain Prediction*:
   - Initially tested a standard RBF kernel, but observed suboptimal performance (R² = 0.85, MSE = 14.90)
   - Switched to a composite kernel: RBF + WhiteKernel to handle noise in the data
   - The WhiteKernel addition significantly improved performance (R² = 0.93, MSE = 6.68)
   - This improvement indicates that the gain data contained measurement noise that needed explicit modeling
   - The kernel hyperparameters were automatically optimized during model fitting
   
   *Random Forest for Power Prediction*:
   - Initially attempted to use GP for power prediction, but observed poor performance (R² = 0.12, MSE = 0.12)
   - Power consumption data exhibited non-Gaussian distribution with complex, non-linear relationships
   - Switched to Random Forest which can handle non-linear relationships without distributional assumptions
   - Applied log transformation to the power values to handle their wide range (spanning several orders of magnitude)
   - Random Forest achieved much better performance (R² = 0.59, MSE = 0.05)
   
   *Evaluation Metrics*:
   - Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values
     - Lower values indicate better prediction accuracy
     - MSE is sensitive to outliers, which helped us identify problematic circuit configurations
   - R² Score (Coefficient of Determination): Represents the proportion of variance in the dependent variable explained by the model
     - Values range from 0 to 1, with 1 indicating perfect prediction
     - An R² of 0.93 for gain prediction means our model explains 93% of the variance in gain values
     - An R² of 0.59 for power prediction means our model explains 59% of the variance in power values
     - R² can be interpreted as how much better the model is compared to simply using the mean value:
       - R² = 0: Model predictions are no better than using the mean value
       - R² = 1: Model perfectly predicts all values
       - Negative R²: Model performs worse than using the mean value
     - Higher R² values indicate that the surrogate model is more reliable for guiding the optimization process
     - The difference in R² between gain and power models (0.93 vs 0.59) explains why our optimization focused more on improving gain prediction accuracy
   - These metrics guided our model selection process and hyperparameter tuning
   - We prioritized models with higher R² scores as they provide better predictive power for optimization
   
   *Model Retraining Strategy*:
   - Models were initially trained on random samples
   - During optimization, models were retrained whenever new valid data points were collected
   - Only valid circuit configurations (positive gain) were used for retraining
   - This adaptive approach improved prediction accuracy as the optimization progressed
   - Retraining was triggered after collecting at least 3 valid points to ensure statistical significance

3. **Bayesian Optimization Implementation**:
   - Developed a custom Expected Improvement (EI) acquisition function combining both surrogate models
   - Added an exploration term (xi=0.01) to balance exploration-exploitation tradeoff
   - Implemented a propose_next_point function that evaluates 10,000 random candidates using the EI acquisition function
   - Created an iterative optimization loop that retrains models as new data becomes available

4. **Standard vs. Custom BO Implementation**:
   - We implemented two different Bayesian Optimization approaches for comparison:
   
   *Standard BO (BO.py)*:
   - Used scikit-optimize (skopt) library's `gp_minimize` function
   - Relied on the built-in Gaussian Process model for all parameters
   - Did not use pre-trained surrogate models
   - Used the standard Expected Improvement (EI) acquisition function
   - Optimization was performed in a "black-box" manner with minimal customization
   - Parameter space was defined using skopt's `Real` space definitions
   
   *Custom BO (custom_bo.py)*:
   - Implemented our own Bayesian Optimization loop from scratch
   - Used separate, specialized surrogate models for gain and power
   - Pre-trained models on initial random samples before optimization
   - Implemented a custom acquisition function combining predictions from both models
   - Retrained models adaptively during optimization
   - Used a more sophisticated candidate point generation and evaluation strategy
   - Provided greater visibility and control over the optimization process

5. **Evaluation and Comparison**:
   - Compared our custom BO approach with:
     - Random sampling
     - Standard BO implementation using scikit-optimize (skopt)
   - Tracked convergence speed and final solution quality

## Results and Analysis
We performed experiments with our custom Bayesian Optimization (custom_bo.py) and compared it with a standard implementation (bo.py) using the following setup:

- 10 initial random points for both approaches
- 50 iterations of Bayesian optimization
- Objective function: Score = -Gain + 0.2·Power
- Evaluation metric: Final best score and convergence rate

### Key Results:
1. **Performance Comparison**:
   - Custom BO achieved a best score of -32.8988
   - Standard BO (using skopt) achieved a best score of -33.6385
   - Random sampling achieved a best score of -34.7783
   - Unexpectedly, random sampling outperformed both BO approaches in terms of final score
   - This suggests that the parameter landscape may have many local optima that are difficult to navigate

2. **Convergence Analysis**:
   - All approaches showed rapid initial improvement within the first 5-10 iterations
   - The convergence plots reveal interesting patterns:
     - Custom BO showed step-wise improvements around iterations 20-25 and 40-45, indicating successful exploitation phases
     - Standard BO (skopt) showed a major improvement around iteration 120, suggesting it required more iterations to escape local optima
     - Random sampling found good solutions very early, outperforming both BO approaches through most of the optimization process
   - Both BO approaches struggled to outperform random sampling, indicating that the circuit parameter space may be particularly challenging for Bayesian optimization

3. **Parameter Configurations**:
   - The best configurations found by each approach differed significantly:
     - W_M5 showed the largest difference (94.43%) between approaches
     - I1 (bias current) differed by 77.20%
     - W_M4 differed by 49.22%
   - These differences suggest that multiple parameter combinations can achieve similar performance
   - The parameter space likely contains multiple local optima with similar scores

4. **Surrogate Model Performance**:
   - Gaussian Process for gain prediction achieved R² scores of 0.93+
   - Random Forest for power prediction achieved R² scores of 0.59+
   - Despite good surrogate model accuracy, BO struggled to outperform random sampling, suggesting limitations in the acquisition function's ability to guide the search effectively

5. **Exploration-Exploitation Balance**:
   - Setting xi=0.01 in the custom BO approach balanced exploration and exploitation
   - The convergence plots show that this helped with periodic improvements in score, but wasn't sufficient to overcome the advantage of random sampling

### Visualizations:
- Generated convergence plots comparing all three approaches (shown above)
- The plots clearly illustrate that:
  1. Random sampling (blue line in second plot) found good solutions very early
  2. Custom BO (orange line) made step-wise improvements but couldn't match random sampling's best score
  3. Standard BO required more iterations (around 120) to find its best solution
- Created parameter importance analysis
- Produced predicted vs. actual performance plots for surrogate model validation

### Unexpected Findings:
- Random sampling outperformed both BO approaches with a score of -34.7783
- This challenges the common assumption that Bayesian optimization always outperforms random search
- The convergence plots suggest that for this specific circuit optimization problem:
  1. The parameter landscape is highly multimodal with many good local optima
  2. Random search was more effective at exploring the full parameter space
  3. The acquisition functions used in both BO approaches may have focused too much on exploiting regions that weren't globally optimal
- This finding highlights the importance of establishing a random sampling baseline before applying more complex optimization methods

## Conclusion
Our custom Bayesian Optimization approach with specialized surrogate models successfully optimized OpAmp circuit parameters, achieving higher gain and lower power consumption compared to traditional methods.

### Key Learnings:
1. The combination of different surrogate models (GP for gain, RF for power) is more effective than using a single model type for all parameters
2. Proper exploration-exploitation balance (xi=0.01) is crucial for efficient optimization
3. Retraining surrogate models with newly acquired valid points improves prediction accuracy throughout the optimization process

### Project Evolution:
The final project expanded beyond our initial proposal by implementing:
- Multiple surrogate model types instead of a single Gaussian Process
- A custom acquisition function that incorporates both models
- A more sophisticated parameter sampling strategy for better exploration

### Challenges and Bottlenecks:
1. **Model Training**: Balancing model complexity with training speed was challenging
2. **Invalid Circuits**: Handling invalid circuit configurations that produced errors in LTSpice
3. **Hyperparameter Tuning**: Finding optimal hyperparameters for both the GP and RF models
4. **Computational Efficiency**: Managing the tradeoff between number of candidate points evaluated and optimization speed

### Future Work:
- Integration with Deep Reinforcement Learning as described in the referenced paper
- Extension to multi-objective optimization with Pareto fronts
- Application to more complex circuit topologies
- Implementation of more advanced acquisition functions like Portfolio strategies



